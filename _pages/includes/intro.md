**Rongjie Huang (ÈªÑËûçÊù∞)** is the final year's graduate student at College of Computer Science and Software, [Zhejiang University](https://www.zju.edu.cn/english/), supervised by [Prof. Zhou Zhao](https://person.zju.edu.cn/zhaozhou). I also obtained Bachelor‚Äôs degree at Zhejiang University. During my graduate study, I was lucky to collaborate with the CMU Speech Team led by [Prof. Shinji Watanabe](https://scholar.google.com/citations?user=U5xRA6QAAAAJ), and Audio Research Team at Zhejiang University. I was grateful to intern or collaborate at TikTok, Shanghai AI Lab, Tencent Seattle Lab, Alibaba Qwen, with [Yi Ren](https://github.com/RayeRen), [Jinglin Liu](https://github.com/MoonInTheRiver), [Chunlei Zhang](https://scholar.google.com/citations?user=NCKZGb0AAAAJ) and [Dong Yu](https://scholar.google.com/citations?user=tMY31_gAAAAJ).

My research interest includes **Multi-Modal Generative AI, Multi-Modal Language Processing, and AI4Science**. I have published **first-author papers** at the top international AI conferences such as **NeurIPS/ICLR/ICML/ACL/IJCAI**. I developed a few well-known Speech/NLP algorithms including:
- AudioGPT, UniAudio, Make-A-Voice: Multitask, Multilingual LLMs
- Make-An-Audio, GenerSpeech: Zero-shot text-guided synthesis
- FastDiff 1/2, ProDiff: AIGC diffusion models
- TranSpeech, and AV-TranSpeech: Multimodal Translation

In 2024, I lead or participate in the following research topics:
- Speech/NLP: multimodal generation and translation
- Large Language Models (LLMs): Audio/Visual
- Diffusion models: Image/Audio/3D


# üî• News

<style>
  .scrollable {
    max-height: 260px; /* ËÆæÁΩÆÊúÄÂ§ßÈ´òÂ∫¶ */
    overflow-y: scroll; /* ËÆæÁΩÆÂûÇÁõ¥ÊªöÂä®Êù° */
  }
</style>

<div class="scrollable">
  <ul>
    <li><strong>2024.05</strong>: 6 papers are accepted by ACL 2024! (main conference and findings)! Thanks to my co-authors! </li>
    <li><strong>2024.05</strong>: 3 papers are accepted by ICML 2024!</li>
    <li><strong>2024.03</strong>: 1 paper is accepted by NAACL 2024 main conference!</li>
    <li><strong>2024.01</strong>: 1 paper is accepted by ICLR 2024!</li>
    <li><strong>2023.11</strong>: 2 papers are accepted by AAAI 2024 main / AAAI 2024 demo!</li>
    <li><strong>2023.10</strong>: <font color="red"> I am awarded ByteDance Scholar Fellowship, and Chu Kochen Presidential Scholarship! </font></li>
    <li><strong>2023.10</strong>: <a href="https://twitter.com/_akhaliq/status/1710112638422642732">UniAudio</a> released!</li>
    <li><strong>2023.09</strong>: One paper is accepted by EMNLP 2023!</li>
    <li><strong>2023.07</strong>: One paper is accepted by ACM-MM 2023! </li>
    <li><strong>2023.06</strong>: One paper is accepted by ICCV 2023! </li>
    <li><strong>2023.05</strong>: 8 papers are accepted by ACL 2023 (main conference and findings)! Thanks to my co-authors! </li>
    <li><strong>2023.04</strong>:  <a href="https://github.com/AIGC-Audio/AudioGPT">AudioGPT</a> and <a href="https://github.com/yangdongchao/AcademiCodec">HiFi-Codec</a> released!  </li>
    <li><strong>2023.04</strong>: One papers is accepted by ICML 2023! </li>
    <li><strong>2023.02</strong>: Make-An-Audio released! Media coverage: <a href="https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw">Heart of Machine</a>, <a href="https://zhuanlan.zhihu.com/p/605228032">ByteDance</a> and <a href="https://twitter.com/_akhaliq/status/1619589070329348096">Twitter</a> </li>
    <li><strong>2023.01</strong>: One papers is accepted by ICLR 2023! </li>
    <li><strong>2022.09</strong>: Two papers are accepted by NeurIPS 2022! </li>
  </ul>
</div>

